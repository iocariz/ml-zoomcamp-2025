{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Production Model Training with Optimized Hyperparameters\n",
    "\n",
    "## MLZoomcamp Capstone Project - Final Model Training\n",
    "\n",
    "---\n",
    "\n",
    "### üìã Overview\n",
    "\n",
    "This notebook trains production-ready models using the optimized hyperparameters from our Optuna optimization. We'll:\n",
    "\n",
    "1. **Load optimized hyperparameters** from the optimization phase\n",
    "2. **Train models with best configurations** for maximum epochs\n",
    "3. **Create optimized ensemble** with best weights\n",
    "4. **Comprehensive evaluation** on test set\n",
    "5. **Save production-ready models** with all artifacts\n",
    "\n",
    "### üéØ Goals\n",
    "\n",
    "- Achieve best possible performance with optimized settings\n",
    "- Create deployment-ready model packages\n",
    "- Generate final performance report\n",
    "- Prepare models for API deployment\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî• Using device: mps\n",
      "‚úÖ Environment setup complete\n",
      "üìÖ Training date: 2025-12-21 21:50:53\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import json\n",
    "import pickle\n",
    "import time\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import hashlib\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Enable MPS fallback for M4 Mac\n",
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n",
    "\n",
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam, AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score,\n",
    "    precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report,\n",
    "    precision_recall_curve, roc_curve\n",
    ")\n",
    "\n",
    "# Model tracking\n",
    "from collections import defaultdict\n",
    "import joblib\n",
    "\n",
    "# Set device for M4 Mac\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f\"üî• Using device: {device}\")\n",
    "\n",
    "# Set random seeds\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Custom colors\n",
    "COLORS = {\n",
    "    'normal': '#2E7D32',\n",
    "    'fraud': '#C62828',\n",
    "    'primary': '#1565C0',\n",
    "    'secondary': '#FF6F00',\n",
    "    'success': '#4CAF50'\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Environment setup complete\")\n",
    "print(f\"üìÖ Training date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data and Optimization Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data loaded: 88 features\n",
      "   Train: 199,364 samples (384 fraud)\n",
      "   Val: 42,721 samples (56 fraud)\n",
      "   Test: 42,722 samples (52 fraud)\n"
     ]
    }
   ],
   "source": [
    "# Load preprocessed data\n",
    "artifacts_dir = Path('artifacts')\n",
    "\n",
    "X_train_scaled = np.load(artifacts_dir / 'X_train_scaled.npy')\n",
    "X_val_scaled = np.load(artifacts_dir / 'X_val_scaled.npy')\n",
    "X_test_scaled = np.load(artifacts_dir / 'X_test_scaled.npy')\n",
    "y_train = np.load(artifacts_dir / 'y_train.npy')\n",
    "y_val = np.load(artifacts_dir / 'y_val.npy')\n",
    "y_test = np.load(artifacts_dir / 'y_test.npy')\n",
    "\n",
    "# Load configuration\n",
    "with open(artifacts_dir / 'config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "n_features = config['n_features']\n",
    "\n",
    "print(f\"‚úÖ Data loaded: {n_features} features\")\n",
    "print(f\"   Train: {X_train_scaled.shape[0]:,} samples ({y_train.sum():,} fraud)\")\n",
    "print(f\"   Val: {X_val_scaled.shape[0]:,} samples ({y_val.sum():,} fraud)\")\n",
    "print(f\"   Test: {X_test_scaled.shape[0]:,} samples ({y_test.sum():,} fraud)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded optimization results\n",
      "\n",
      "üìä Hyperparameters to use:\n",
      "   Autoencoder: 24D encoding_dim\n",
      "   VAE: 16D latent space\n",
      "   SVDD: 8D representation space\n",
      "   Threshold: 96.52521733675086th percentile\n"
     ]
    }
   ],
   "source": [
    "# Load optimization results (if available)\n",
    "optim_dir = Path('optimization_results')\n",
    "\n",
    "if optim_dir.exists() and (optim_dir / 'optimization_results.json').exists():\n",
    "    with open(optim_dir / 'optimization_results.json', 'r') as f:\n",
    "        optim_results = json.load(f)\n",
    "    print(\"‚úÖ Loaded optimization results\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No optimization results found, using default hyperparameters\")\n",
    "    # Default hyperparameters\n",
    "    optim_results = {\n",
    "        'autoencoder': {\n",
    "            'best_params': {\n",
    "                'encoding_dim': 32,\n",
    "                'n_layers': 2,\n",
    "                'hidden_dim_0': 64,\n",
    "                'hidden_dim_1': 48,\n",
    "                'dropout_rate': 0.2,\n",
    "                'activation': 'relu',\n",
    "                'batch_size': 256,\n",
    "                'learning_rate': 0.001,\n",
    "                'optimizer': 'adamw'\n",
    "            }\n",
    "        },\n",
    "        'vae': {\n",
    "            'best_params': {\n",
    "                'latent_dim': 20,\n",
    "                'n_layers': 2,\n",
    "                'hidden_dim_0': 64,\n",
    "                'hidden_dim_1': 48,\n",
    "                'dropout_rate': 0.2,\n",
    "                'activation': 'relu',\n",
    "                'beta': 1.0,\n",
    "                'batch_size': 256,\n",
    "                'learning_rate': 0.001,\n",
    "                'optimizer': 'adamw'\n",
    "            }\n",
    "        },\n",
    "        'svdd': {\n",
    "            'best_params': {\n",
    "                'rep_dim': 20,\n",
    "                'n_layers': 2,\n",
    "                'hidden_dim_0': 64,\n",
    "                'hidden_dim_1': 48,\n",
    "                'dropout_rate': 0.2,\n",
    "                'activation': 'relu',\n",
    "                'batch_size': 256,\n",
    "                'learning_rate': 0.001,\n",
    "                'optimizer': 'adamw'\n",
    "            }\n",
    "        },\n",
    "        'ensemble': {\n",
    "            'best_weights': [0.3, 0.4, 0.3]\n",
    "        },\n",
    "        'threshold': {\n",
    "            'best_percentile': 95.0\n",
    "        }\n",
    "    }\n",
    "\n",
    "print(\"\\nüìä Hyperparameters to use:\")\n",
    "print(f\"   Autoencoder: {optim_results['autoencoder']['best_params'].get('encoding_dim', 32)}D encoding_dim\")\n",
    "print(f\"   VAE: {optim_results['vae']['best_params'].get('latent_dim', 20)}D latent space\")\n",
    "print(f\"   SVDD: {optim_results['svdd']['best_params'].get('rep_dim', 20)}D representation space\")\n",
    "print(f\"   Threshold: {optim_results['threshold'].get('best_percentile', 95.0)}th percentile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì¶ Datasets ready:\n",
      "   Normal training: 198,980 samples\n",
      "   All training: 199,364 samples\n",
      "   Validation: 42,721 samples\n",
      "   Test: 42,722 samples\n"
     ]
    }
   ],
   "source": [
    "# Create PyTorch datasets\n",
    "class FraudDataset(Dataset):\n",
    "    def __init__(self, X, y=None):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.FloatTensor(y) if y is not None else None\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is not None:\n",
    "            return self.X[idx], self.y[idx]\n",
    "        return self.X[idx]\n",
    "\n",
    "# Create datasets for normal training samples only\n",
    "normal_idx = y_train == 0\n",
    "X_train_normal = X_train_scaled[normal_idx]\n",
    "y_train_normal = y_train[normal_idx]\n",
    "\n",
    "train_dataset_normal = FraudDataset(X_train_normal, y_train_normal)\n",
    "train_dataset_all = FraudDataset(X_train_scaled, y_train)\n",
    "val_dataset = FraudDataset(X_val_scaled, y_val)\n",
    "test_dataset = FraudDataset(X_test_scaled, y_test)\n",
    "\n",
    "print(f\"\\nüì¶ Datasets ready:\")\n",
    "print(f\"   Normal training: {len(train_dataset_normal):,} samples\")\n",
    "print(f\"   All training: {len(train_dataset_all):,} samples\")\n",
    "print(f\"   Validation: {len(val_dataset):,} samples\")\n",
    "print(f\"   Test: {len(test_dataset):,} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Optimized Model Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedAutoencoder(nn.Module):\n",
    "    \"\"\"Autoencoder with optimized architecture\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, params):\n",
    "        super().__init__()\n",
    "        \n",
    "        encoding_dim = params.get('encoding_dim', 32)\n",
    "        dropout_rate = params.get('dropout_rate', 0.2)\n",
    "        activation = params.get('activation', 'relu')\n",
    "        \n",
    "        # Build hidden layers from params\n",
    "        hidden_layers = []\n",
    "        n_layers = params.get('n_layers', 2)\n",
    "        for i in range(n_layers):\n",
    "            hidden_dim = params.get(f'hidden_dim_{i}', 64)\n",
    "            hidden_layers.append(hidden_dim)\n",
    "        \n",
    "        # Select activation\n",
    "        if activation == 'relu':\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation == 'leaky_relu':\n",
    "            self.activation = nn.LeakyReLU(0.1)\n",
    "        elif activation == 'elu':\n",
    "            self.activation = nn.ELU()\n",
    "        else:\n",
    "            self.activation = nn.Tanh()\n",
    "        \n",
    "        # Encoder\n",
    "        encoder_layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_layers:\n",
    "            encoder_layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                self.activation,\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        encoder_layers.append(nn.Linear(prev_dim, encoding_dim))\n",
    "        self.encoder = nn.Sequential(*encoder_layers)\n",
    "        \n",
    "        # Decoder (mirror)\n",
    "        decoder_layers = []\n",
    "        prev_dim = encoding_dim\n",
    "        \n",
    "        for hidden_dim in reversed(hidden_layers):\n",
    "            decoder_layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                self.activation,\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        decoder_layers.append(nn.Linear(prev_dim, input_dim))\n",
    "        self.decoder = nn.Sequential(*decoder_layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "    \n",
    "    def get_embedding(self, x):\n",
    "        return self.encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedVAE(nn.Module):\n",
    "    \"\"\"VAE with optimized architecture\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, params):\n",
    "        super().__init__()\n",
    "        \n",
    "        latent_dim = params.get('latent_dim', 20)\n",
    "        dropout_rate = params.get('dropout_rate', 0.2)\n",
    "        activation = params.get('activation', 'relu')\n",
    "        \n",
    "        # Build hidden layers\n",
    "        hidden_layers = []\n",
    "        n_layers = params.get('n_layers', 2)\n",
    "        for i in range(n_layers):\n",
    "            hidden_dim = params.get(f'hidden_dim_{i}', 64)\n",
    "            hidden_layers.append(hidden_dim)\n",
    "        \n",
    "        # Select activation\n",
    "        if activation == 'relu':\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation == 'leaky_relu':\n",
    "            self.activation = nn.LeakyReLU(0.1)\n",
    "        else:\n",
    "            self.activation = nn.ELU()\n",
    "        \n",
    "        # Encoder\n",
    "        encoder_layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_layers:\n",
    "            encoder_layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                self.activation,\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        self.encoder = nn.Sequential(*encoder_layers)\n",
    "        self.fc_mu = nn.Linear(prev_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(prev_dim, latent_dim)\n",
    "        \n",
    "        # Decoder\n",
    "        decoder_layers = []\n",
    "        prev_dim = latent_dim\n",
    "        \n",
    "        for hidden_dim in reversed(hidden_layers):\n",
    "            decoder_layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                self.activation,\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        decoder_layers.append(nn.Linear(prev_dim, input_dim))\n",
    "        self.decoder = nn.Sequential(*decoder_layers)\n",
    "        \n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        logvar = torch.clamp(logvar, min=-20, max=2)\n",
    "        return mu, logvar\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        recon = self.decode(z)\n",
    "        return recon, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedDeepSVDD(nn.Module):\n",
    "    \"\"\"Deep SVDD network for tabular anomaly detection\"\"\"\n",
    "    def __init__(self, input_dim, params):\n",
    "        super().__init__()\n",
    "        rep_dim = params.get('rep_dim', 16)\n",
    "        dropout_rate = params.get('dropout_rate', 0.2)  \n",
    "        activation = params.get('activation', 'relu')\n",
    "        \n",
    "        # Select activation\n",
    "        if activation == 'relu':\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation == 'leaky_relu':       \n",
    "            self.activation = nn.LeakyReLU(0.1)\n",
    "        elif activation == 'elu':\n",
    "            self.activation = nn.ELU()\n",
    "        else:\n",
    "            self.activation = nn.Tanh()\n",
    "\n",
    "        layers = []\n",
    "        prev = input_dim    \n",
    "\n",
    "        # Build hidden layers from params\n",
    "        n_layers = params.get('n_layers', 2)\n",
    "        # If params doesn't have hidden_dim_i, check for 'hidden_layers' list fallback\n",
    "        if not any(f'hidden_dim_{i}' in params for i in range(n_layers)) and 'hidden_layers' in params:\n",
    "            hidden_dims = params['hidden_layers']\n",
    "        else:\n",
    "            hidden_dims = [params.get(f'hidden_dim_{i}', 64) for i in range(n_layers)]\n",
    "\n",
    "        for h in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev, h),\n",
    "                nn.BatchNorm1d(h),\n",
    "                self.activation,\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ])\n",
    "            prev = h\n",
    "            \n",
    "        layers.append(nn.Linear(prev, rep_dim))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "@torch.no_grad()\n",
    "def init_svdd_center(model, loader, device, eps=1e-3):\n",
    "    model.eval()\n",
    "    reps = []\n",
    "    for x, _ in loader:\n",
    "        x = x.to(device)\n",
    "        z = model(x)\n",
    "        reps.append(z.detach().cpu())\n",
    "    reps = torch.cat(reps, dim=0)\n",
    "    c = reps.mean(dim=0)\n",
    "    c[(c.abs() < eps)] = eps * c[(c.abs() < eps)].sign().clamp(min=1)\n",
    "    return c.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Enhanced Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductionTrainer:\n",
    "    \"\"\"Enhanced trainer for production models (AE / VAE / DeepSVDD)\"\"\"\n",
    "\n",
    "    def __init__(self, model, model_type=\"autoencoder\", params=None, device=None):\n",
    "        self.device = device or torch.device(\n",
    "            \"mps\" if torch.backends.mps.is_available()\n",
    "            else \"cuda\" if torch.cuda.is_available()\n",
    "            else \"cpu\"\n",
    "        )\n",
    "\n",
    "        self.model = model.to(self.device)\n",
    "        self.model_type = model_type\n",
    "        self.params = params or {}\n",
    "\n",
    "        # Optimizer\n",
    "        lr = float(self.params.get(\"learning_rate\", 0.001))\n",
    "        opt_name = str(self.params.get(\"optimizer\", \"adamw\")).lower()\n",
    "\n",
    "        if opt_name == \"adamw\":\n",
    "            self.optimizer = AdamW(self.model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "        else:\n",
    "            self.optimizer = Adam(self.model.parameters(), lr=lr)\n",
    "\n",
    "        # Scheduler\n",
    "        self.scheduler = ReduceLROnPlateau(self.optimizer, mode=\"min\", patience=10, factor=0.5)\n",
    "\n",
    "        # History tracking\n",
    "        self.history = defaultdict(list)\n",
    "        self.best_model_state = None\n",
    "        self.best_metrics = {}\n",
    "\n",
    "        # Deep SVDD center (trainer-level fallback)\n",
    "        self.center_c = None\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def init_svdd_center(self, train_loader, eps=1e-3, max_batches=None):\n",
    "        \"\"\"\n",
    "        Initialize SVDD center c as mean representation over training data.\n",
    "        Assumes the loader contains NORMAL transactions only (recommended).\n",
    "\n",
    "        Assumes the loader contains NORMAL transactions only (recommended).\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        reps = []\n",
    "        for b_idx, data in enumerate(train_loader):\n",
    "            if max_batches is not None and b_idx >= max_batches:\n",
    "                break\n",
    "            if isinstance(data, (list, tuple)):\n",
    "                x = data[0]\n",
    "            else:\n",
    "                x = data\n",
    "            x = x.to(self.device)\n",
    "\n",
    "            z = self.model(x)  # DeepSVDD forward returns representation\n",
    "            reps.append(z.detach().cpu())\n",
    "\n",
    "        if len(reps) == 0:\n",
    "            raise ValueError(\"Could not initialize SVDD center: empty loader?\")\n",
    "        reps = torch.cat(reps, dim=0)\n",
    "        c = reps.mean(dim=0)\n",
    "\n",
    "        # avoid exactly-0 dims\n",
    "        mask = c.abs() < eps\n",
    "        if mask.any():\n",
    "            c[mask] = eps * torch.sign(c[mask]).clamp(min=1)\n",
    "\n",
    "        # Prefer model buffer if present\n",
    "        if hasattr(self.model, \"center_c\") and torch.is_tensor(getattr(self.model, \"center_c\")):\n",
    "            self.model.center_c.data = c.to(self.device)\n",
    "        else:\n",
    "            # fallback\n",
    "            self.center_c = c.to(self.device)\n",
    "\n",
    "    def _svdd_center(self):\n",
    "        if hasattr(self.model, \"center_c\") and torch.is_tensor(getattr(self.model, \"center_c\")):\n",
    "            return self.model.center_c\n",
    "        if self.center_c is None:\n",
    "            raise RuntimeError(\"SVDD center not initialized. Call init_svdd_center() before training/eval.\")\n",
    "        return self.center_c\n",
    "\n",
    "    def train_epoch(self, train_loader):\n",
    "        self.model.train()\n",
    "        total_loss = 0.0\n",
    "        batch_count = 0\n",
    "        for batch_idx, data in enumerate(train_loader):\n",
    "            if isinstance(data, (list, tuple)):\n",
    "                x = data[0]  # (x, y) or (x, _)\\n\n",
    "            else:\n",
    "                x = data\n",
    "\n",
    "            x = x.to(self.device)\n",
    "            self.optimizer.zero_grad()\n",
    "            if self.model_type == \"vae\":\n",
    "                recon_x, mu, logvar = self.model(x)\n",
    "                beta = float(self.params.get(\"beta\", 1.0))\n",
    "                recon_loss = F.mse_loss(recon_x, x, reduction=\"sum\")\n",
    "                kld_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp().clamp(max=1e10))\n",
    "                loss = (recon_loss + beta * kld_loss) / x.size(0)\n",
    "\n",
    "            elif self.model_type == \"svdd\":\n",
    "                z = self.model(x)\n",
    "                c = self._svdd_center()\n",
    "                loss = torch.mean(torch.sum((z - c) ** 2, dim=1))\n",
    "\n",
    "            else:  # autoencoder\n",
    "                recon_x = self.model(x)\n",
    "                loss = F.mse_loss(recon_x, x)\n",
    "\n",
    "            if torch.isnan(loss) or torch.isinf(loss):\n",
    "                continue\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            self.optimizer.step()\n",
    "            total_loss += float(loss.item())\n",
    "            batch_count += 1\n",
    "        return total_loss / max(batch_count, 1)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _batch_scores(self, x):\n",
    "        \"\"\"\n",
    "        Returns anomaly scores for a batch.\n",
    "        Higher score = more anomalous (consistent across models).\n",
    "        \"\"\"\n",
    "        if self.model_type == \"vae\":\n",
    "            recon_x, _, _ = self.model(x)\n",
    "            scores = F.mse_loss(recon_x, x, reduction=\"none\").mean(dim=1)\n",
    "\n",
    "        elif self.model_type in [\"svdd\", \"deep_svdd\"]:\n",
    "            z = self.model(x)\n",
    "            c = self._svdd_center()\n",
    "            scores = torch.sum((z - c) ** 2, dim=1)\n",
    "\n",
    "        else:\n",
    "            recon_x = self.model(x)\n",
    "            scores = F.mse_loss(recon_x, x, reduction=\"none\").mean(dim=1)\n",
    "\n",
    "        # stabilize\n",
    "        scores = torch.nan_to_num(scores, nan=1000.0, posinf=1000.0, neginf=0.0)\n",
    "        return scores\n",
    "\n",
    "    def evaluate(self, data_loader):\n",
    "        self.model.eval()\n",
    "        all_scores = []\n",
    "        all_labels = []\n",
    "        total_loss = 0.0\n",
    "        batch_count = 0\n",
    "\n",
    "        for data in data_loader:\n",
    "            x, y = data\n",
    "            x = x.to(self.device)\n",
    "\n",
    "            scores = self._batch_scores(x)\n",
    "            scores_np = scores.detach().cpu().numpy()\n",
    "            scores_np = np.clip(scores_np, 0, 1000)\n",
    "\n",
    "            all_scores.extend(scores_np)\n",
    "            all_labels.extend(y.numpy())\n",
    "\n",
    "            total_loss += float(scores.mean().item())\n",
    "            batch_count += 1\n",
    "\n",
    "        all_labels = np.asarray(all_labels).astype(int)\n",
    "        all_scores = np.asarray(all_scores).astype(float)\n",
    "        metrics = {}\n",
    "        if len(np.unique(all_labels)) > 1:\n",
    "            try:\n",
    "                metrics[\"auroc\"] = float(roc_auc_score(all_labels, all_scores))\n",
    "                metrics[\"auprc\"] = float(average_precision_score(all_labels, all_scores))\n",
    "\n",
    "                threshold_pct = float(self.params.get(\"threshold_percentile\", 95))\n",
    "                threshold = float(np.percentile(all_scores[all_labels == 0], threshold_pct))\n",
    "                predictions = (all_scores > threshold).astype(int)\n",
    "\n",
    "                metrics[\"precision\"] = float(precision_score(all_labels, predictions, zero_division=0))\n",
    "                metrics[\"recall\"] = float(recall_score(all_labels, predictions, zero_division=0))\n",
    "                metrics[\"f1\"] = float(f1_score(all_labels, predictions, zero_division=0))\n",
    "                metrics[\"threshold\"] = threshold\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not compute metrics: {e}\")\n",
    "                metrics = {\"auroc\": 0.0, \"auprc\": 0.0, \"precision\": 0.0, \"recall\": 0.0, \"f1\": 0.0}\n",
    "\n",
    "        avg_loss = total_loss / max(batch_count, 1)\n",
    "        return avg_loss, metrics, all_scores, all_labels\n",
    "    def train(self, train_loader, val_loader, epochs=100, patience=20, verbose=True):\n",
    "        \"\"\"\n",
    "        NOTE: For DeepSVDD you should pass train_loader built ONLY from normal transactions.\n",
    "        \"\"\"\n",
    "        # Initialize SVDD center once (before training)\n",
    "        if self.model_type in [\"svdd\", \"deep_svdd\"]:\n",
    "            try:\n",
    "                _ = self._svdd_center()\n",
    "            except Exception:\n",
    "                self.init_svdd_center(train_loader)\n",
    "\n",
    "        best_auroc = 0.0\n",
    "        patience_counter = 0\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            start_time = time.time()\n",
    "            train_loss = self.train_epoch(train_loader)\n",
    "            val_loss, val_metrics, _, _ = self.evaluate(val_loader)\n",
    "            epoch_time = time.time() - start_time\n",
    "            self.scheduler.step(val_loss)\n",
    "\n",
    "            # history\n",
    "            self.history[\"epoch\"].append(epoch + 1)\n",
    "            self.history[\"train_loss\"].append(train_loss)\n",
    "            self.history[\"val_loss\"].append(val_loss)\n",
    "            for k, v in val_metrics.items():\n",
    "                self.history[f\"val_{k}\"].append(v)\n",
    "            self.history[\"epoch_time\"].append(epoch_time)\n",
    "\n",
    "            current_auroc = float(val_metrics.get(\"auroc\", 0.0))\n",
    "\n",
    "            if current_auroc > best_auroc:\n",
    "                best_auroc = current_auroc\n",
    "                self.best_model_state = {k: v.detach().cpu().clone() for k, v in self.model.state_dict().items()}\n",
    "                self.best_metrics = val_metrics.copy()\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "\n",
    "            if verbose and (epoch + 1) % 5 == 0:\n",
    "                print(\n",
    "                    f\"Epoch [{epoch+1:3d}/{epochs}] | \"\n",
    "                    f\"Train Loss: {train_loss:.6f} | \"\n",
    "                    f\"Val Loss: {val_loss:.6f} | \"\n",
    "                    f\"AUROC: {current_auroc:.4f} | \"\n",
    "                    f\"Time: {epoch_time:.1f}s\"\n",
    "                )\n",
    "            if patience_counter >= patience:\n",
    "                if verbose:\n",
    "                    print(f\"\\nEarly stopping at epoch {epoch + 1}\")\n",
    "                break\n",
    "\n",
    "            if self.device.type == \"mps\" and epoch % 10 == 0:\n",
    "                torch.mps.empty_cache()\n",
    "\n",
    "        if self.best_model_state:\n",
    "            self.model.load_state_dict(self.best_model_state)\n",
    "        return self.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train Optimized Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Train Optimized Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Training Optimized Autoencoder\n",
      "============================================================\n",
      "Model parameters: 25,648\n",
      "Epoch [  5/100] | Train Loss: 313.623086 | Val Loss: 35.492490 | AUROC: 0.9584 | Time: 4.1s\n",
      "Epoch [ 10/100] | Train Loss: 309.191105 | Val Loss: 35.663969 | AUROC: 0.9508 | Time: 4.1s\n",
      "Epoch [ 15/100] | Train Loss: 305.981955 | Val Loss: 34.173264 | AUROC: 0.9540 | Time: 3.9s\n",
      "Epoch [ 20/100] | Train Loss: 303.378719 | Val Loss: 33.465468 | AUROC: 0.9547 | Time: 4.1s\n",
      "\n",
      "Early stopping at epoch 24\n",
      "\n",
      "‚úÖ Best AUROC: 0.9599\n",
      "‚úÖ Best F1: 0.0446\n"
     ]
    }
   ],
   "source": [
    "print(\"üöÄ Training Optimized Autoencoder\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get parameters\n",
    "ae_params = optim_results['autoencoder']['best_params']\n",
    "batch_size = ae_params.get('batch_size', 256)\n",
    "\n",
    "# Create model\n",
    "autoencoder_model = OptimizedAutoencoder(n_features, ae_params)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in autoencoder_model.parameters()):,}\")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset_normal, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "# Train\n",
    "ae_trainer = ProductionTrainer(autoencoder_model, model_type='autoencoder', params=ae_params)\n",
    "ae_history = ae_trainer.train(\n",
    "    train_loader, val_loader,\n",
    "    epochs=100,\n",
    "    patience=20,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Best AUROC: {ae_trainer.best_metrics.get('auroc', 0):.4f}\")\n",
    "print(f\"‚úÖ Best F1: {ae_trainer.best_metrics.get('f1', 0):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Train Optimized VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Training Optimized VAE\n",
      "============================================================\n",
      "Model parameters: 25,656\n",
      "Epoch [  5/100] | Train Loss: 25385.298143 | Val Loss: 26.763980 | AUROC: 0.9538 | Time: 4.7s\n",
      "Epoch [ 10/100] | Train Loss: 24233.465621 | Val Loss: 10.681532 | AUROC: 0.9512 | Time: 4.7s\n",
      "Epoch [ 15/100] | Train Loss: 23577.098972 | Val Loss: 7.277839 | AUROC: 0.9575 | Time: 4.6s\n",
      "Epoch [ 20/100] | Train Loss: 22944.951773 | Val Loss: 13.407018 | AUROC: 0.9530 | Time: 4.6s\n",
      "Epoch [ 25/100] | Train Loss: 22625.811007 | Val Loss: 10.215022 | AUROC: 0.9489 | Time: 4.7s\n",
      "Epoch [ 30/100] | Train Loss: 22190.093313 | Val Loss: 16.284673 | AUROC: 0.9483 | Time: 4.7s\n",
      "Epoch [ 35/100] | Train Loss: 22078.887454 | Val Loss: 10.686662 | AUROC: 0.9507 | Time: 4.6s\n",
      "Epoch [ 40/100] | Train Loss: 21861.115612 | Val Loss: 15.140689 | AUROC: 0.9505 | Time: 4.6s\n",
      "\n",
      "Early stopping at epoch 42\n",
      "\n",
      "‚úÖ Best AUROC: 0.9576\n",
      "‚úÖ Best F1: 0.0446\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüöÄ Training Optimized VAE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get parameters\n",
    "vae_params = optim_results['vae']['best_params']\n",
    "batch_size = vae_params.get('batch_size', 256)\n",
    "\n",
    "# Create model\n",
    "vae_model = OptimizedVAE(n_features, vae_params)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in vae_model.parameters()):,}\")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset_normal, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "# Train\n",
    "vae_trainer = ProductionTrainer(vae_model, model_type='vae', params=vae_params)\n",
    "vae_history = vae_trainer.train(\n",
    "    train_loader, val_loader,\n",
    "    epochs=100,\n",
    "    patience=20,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Best AUROC: {vae_trainer.best_metrics.get('auroc', 0):.4f}\")\n",
    "print(f\"‚úÖ Best F1: {vae_trainer.best_metrics.get('f1', 0):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Train Deep SVDD\n",
    "\n",
    "Deep SVDD is trained on **normal** transactions only. We initialize the center `c` from the initial representations and optimize squared distance to `c`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Training Optimized SVDD\n",
      "============================================================\n",
      "Model parameters: 5,672\n",
      "Epoch [  5/100] | Train Loss: 0.003449 | Val Loss: 0.000019 | AUROC: 0.9520 | Time: 2.5s\n",
      "Epoch [ 10/100] | Train Loss: 0.000002 | Val Loss: 0.000000 | AUROC: 0.9395 | Time: 2.5s\n",
      "Epoch [ 15/100] | Train Loss: 0.000000 | Val Loss: 0.000000 | AUROC: 0.7145 | Time: 2.5s\n",
      "Epoch [ 20/100] | Train Loss: 0.000000 | Val Loss: 0.000000 | AUROC: 0.7472 | Time: 2.5s\n",
      "Epoch [ 25/100] | Train Loss: 0.000001 | Val Loss: 0.000000 | AUROC: 0.9063 | Time: 2.5s\n",
      "\n",
      "Early stopping at epoch 28\n",
      "\n",
      "‚úÖ Best AUROC: 0.9613\n",
      "‚úÖ Best F1: 0.0446\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüöÄ Training Optimized SVDD\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get parameters\n",
    "svdd_params = optim_results['svdd']['best_params']\n",
    "batch_size = svdd_params.get('batch_size', 256)\n",
    "\n",
    "# Create model\n",
    "svdd_model = OptimizedDeepSVDD(input_dim=n_features, params=svdd_params).to(device)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in svdd_model.parameters()):,}\")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset_normal, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "# Train\n",
    "svdd_trainer = ProductionTrainer(svdd_model, model_type='svdd', params=svdd_params)\n",
    "svdd_trainer.init_svdd_center(train_loader)\n",
    "svdd_history = svdd_trainer.train(\n",
    "    train_loader, val_loader,\n",
    "    epochs=100,\n",
    "    patience=20,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Best AUROC: {svdd_trainer.best_metrics.get('auroc', 0):.4f}\")\n",
    "print(f\"‚úÖ Best F1: {svdd_trainer.best_metrics.get('f1', 0):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Final Model Comparison on Test Set\n",
      "======================================================================\n",
      "      Model  AUROC  AUPRC  Precision  Recall     F1\n",
      "Autoencoder 0.9190 0.0813     0.0198  0.8269 0.0386\n",
      "        VAE 0.9551 0.1044     0.0198  0.8269 0.0386\n",
      "       SVDD 0.9428 0.3078     0.0198  0.8269 0.0386\n",
      "\n",
      "üèÜ Best Model: VAE (AUROC=0.9551)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate all models on test set\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False, num_workers=0)\n",
    "\n",
    "print(\"\\nüìä Final Model Comparison on Test Set\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "comparison_results = []\n",
    "\n",
    "# Evaluate individual models\n",
    "for name, trainer in [('Autoencoder', ae_trainer), ('VAE', vae_trainer), ('SVDD', svdd_trainer)]:\n",
    "    _, metrics, scores, labels = trainer.evaluate(test_loader)\n",
    "    \n",
    "    comparison_results.append({\n",
    "        'Model': name,\n",
    "        'AUROC': metrics.get('auroc', 0),\n",
    "        'AUPRC': metrics.get('auprc', 0),\n",
    "        'Precision': metrics.get('precision', 0),\n",
    "        'Recall': metrics.get('recall', 0),\n",
    "        'F1': metrics.get('f1', 0)\n",
    "    })\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison_df = pd.DataFrame(comparison_results)\n",
    "comparison_df = comparison_df.round(4)\n",
    "\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Find best model\n",
    "best_idx = comparison_df['AUROC'].idxmax()\n",
    "print(f\"\\nüèÜ Best Model: {comparison_df.loc[best_idx, 'Model']} (AUROC={comparison_df.loc[best_idx, 'AUROC']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch 3.11)",
   "language": "python",
   "name": "torch311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
